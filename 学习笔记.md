# 番茄鱼的机器学习的学习笔记
目录
- [数学知识](./数学知识.md)
- [VAE相关](./VAE相关.md)
- [极值理论](./极值理论.md)

### Soft-IntroVAE推导


## 基于最优化方法的机器学习套路

### 背景：算法分类和最优化
分类和预测问题可以看成是拟合函数，而生成模型问题则认为是拟合分布。拟合分布有多种方法，VAE以重构为目标，GAN以鉴别为目标，另外还可以基于强化学习以及演化学习。

然而无论哪种方法，最终需要体现为定义合适的目标函数（最小化/最大化），使用泛函优化（概率分布是一个函数）的数学技巧，将难以优化的泛函优化问题变成可以优化的损失函数和神经网络。

要注意损失函数与需要优化的目标函数的区别。优化目标函数很多时候不能直接计算，需要转换，而损失函数必须可以计算。

因此机器学习的套路是：需求分析->目标函数->损失函数->网络设计->数据收集->调参优化

### 仔细分析需求确定优化的目标函数
例子：深度学习的互信息：无监督提取特征
https://kexue.fm/archives/6024

区分不同分布是目标，但重构数据不是目标，而且两者还不等价！

### 拟合分布的函数类型
神经网络是一个参数化的函数。这个函数的输入如果满足某个概率分布，则输出的值会满足另外一个分布。如果类型系统携带上概率分布的信息，可以作为文档和推理检验的有用信息。

具体表现：输入可以是满足某个分布的向量，而输出则是想计算得到的目标分布的向量。
$$
T: \\{ P: Probability,Q: Vec(R,n) \to Probability \\} \to (x:P) \to (y:Q(x))
$$
其中P和Q都是表示概率分布，生成代码时可以删除。

## 数据增强/提升框架
[参考]
(https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&mid=2247653464&idx=1&sn=4cea82edf390d2b9b7feecba32733963&scene=58&subscene=0&exportkey=A%2FeVd4jwuypjJBkWq066NEs%3D&pass_ticket=xY8f5SA8zxXMB7wrWPl0IfIfkbG5f5ayTp8t4lf4funRXPdHy3mGaF2ZlfV76GQC&wx_header=0  "关注数据而不是模型：我是如何赢得吴恩达首届 Data-centric AI 竞赛的")

[英文连接][how-i-won-andrew-ngs-very-first-data-centric-ai-competition]

得奖的方法如下：

1. 通过训练数据得到生成模型，并生成一定数量的候选增强数据
2. 划分训练和验证数据集合，训练预测模型并预测验证数据的标签
3. 使用另外一个模型对图像数据（训练数据和增强数据）进行嵌入，得到类似词向量的模型 
4. 对每个预测错误的验证数据，利用图形嵌入向量使用余弦相似度从增强数据中查找临近点对应的增强数据，并加入到训练集合中
5. 重新训练预测模型，并预测验证集的标签
6. 重复4-6步，知道增强数据的数量达到上限

以上过程可以抽象为：

1. 针对数据特征确定生成-聚类模型，例如VAE聚类模型
2. 使用全部数据训练生成-聚类模型
3. 划分训练和验证数据集合，保证每个类别都有训练数据和验证数据
4. 训练预测模型，预测验证集标签
5. 对每个预测错误的数据，根据类别使用生成模型生成增强数据若干个，并加入到训练数据集合
6. 重复4-5步骤，知道满足终止条件（例如增强数据的数量达到上限，或则运行次数已经达到上限）

得奖方法把生成-聚类模型拆分为生成模型和对象嵌入模型，并使用Annoy算法包执行临近搜索。如果对象嵌入模型比较难设计，而数据分类的数量可以直接通过分析数据特征得到，通过VAE-Clustering算法模型可能更好，需要进一步探索。

[模型参考](https://kexue.fm/archives/5887 "变分自编码器（四）：一步到位的聚类方案")

在[VAE相关](./VAE相关.md)的VAE聚类模型小节有详细推导

[how-i-won-andrew-ngs-very-first-data-centric-ai-competition]: https://towardsdatascience.com/how-i-won-andrew-ngs-very-first-data-centric-ai-competition-e02001268bda  "how-i-won-andrew-ngs-very-first-data-centric-ai-competition"


## 可逆概率分布拟合模型 Normalized FLOW

## GAN原理

### f-GAN

### 对偶空间GAN

### 一般GAN

### GAN与VAE比较
虽然都可以通过变分推断推导，但是GAN假设的分布比VAE少。

## VAE目标函数分解为编码器部分以及解码器部分

### KL散度与ELBO

## 信息论知识

### 最小交叉熵原理

#### 特例：最大熵原理

#### 特例：EM算法
部分参考 https://kexue.fm/archives/5239

## 变分推断统一框架下的VAE/GAN/EM算法
部分参考 https://kexue.fm/archives/5716

都是概率估算在生成模型中的应用

## RNN系列与自动机

## GRU

$$
\begin{split}
reset_t & = \sigma(W^{rh} state_{t-1} + W^{rx} x_t + b^r) \quad reset \ old \ state \ in \ new \ cell   \\\\
update_t & = \sigma(W^{uh} state_{t-1} + W^{ux} x_t + b^u) \quad update  \ old \ state \ in \ new \ state \\\\
cell_t & = tanh(W^{ch} (state_{t-1} \circ reset_t) + W^{cx} x_t + b^c) \quad cell \ is \ candidate \\\\
state_t & = update_t \circ state_{t-1} + (1-update_t) \circ cell_t
\end{split}
$$

## Program Synthesis
我的想法：
+ Problem Description
+ test cases
+ formal specification
+ concept/instance relation analysis (knowledge graph reasoning)
+ data structure analysis (graph similarity search)
+ partial program (sampling)
+ refinement of partial program (attention of which whole to fill)
+ bounded recursive local search: random search/systematic typed search/api search
+ abstraction (reasoning)
+ test and feed back(reasoning/counter example guided): backtrack point -> data structure/partial program, make sure no duplication is searched

收集训练数据是一个难点
* 第一步可以尝试教科书和网站上的题目，后续可以尝试强化学习生成？
* 训练数据需要进行合适的预处理
* 训练数据可以把预测数据结构考虑进去，作为上下文之一，甚至可以根据搜索过程的状态进行（增量）预测。
* 前文向量可以包含三部分，第一是描述，第二是数据结构，第三是当前模板（partial program）；可以使用语言模型的方法进行嵌入，可以针对数据结构（图），模板（树）进行特别设计的嵌入。
* 预处理可以将各个策略都跑一遍(sampling/refinement/random search/systematic typed  search/api search)，每个策略的最优参数作为新的训练目标。
* 更进一步的，可以将每个正确代码拆分成一个一个的模板（partial program），然后每个模板都进行策略搜索，确定对应的最优参数，将模板也加入到前文向量，就得到更多的训练数据。
* 问题大概可以看成在合适的前文（向量表示）中确定各个参数的联合分布。
* 完整或部分程序运行过程的状态序列如何反馈到搜索策略中还是一个未知数。
* 如何把程序类型作为上下文嵌入向量还是一个未知数。
* 语法和语义规则如何保障还是一个未知数。